import os
import datetime
import argparse
import akshare as ak
import pandas as pd
import markdown
import json
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential

# é…ç½® Azure OpenAI
def load_config():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, "config.json")
    if os.path.exists(config_path):
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    return {}

AZURE_CONFIG = load_config()

# å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ç¼ºå°‘å…³é”®å­—æ®µï¼Œæä¾›é»˜è®¤å€¼ï¼ˆä¸å«æ•æ„Ÿä¿¡æ¯ï¼‰
if not AZURE_CONFIG:
    AZURE_CONFIG = {
        "deploymentName": "gpt-4.1-mini",
        "maxTokens": 3000,
        "temperature": 0.7
    }

def get_azure_client():
    try:
        if "apiKey" in AZURE_CONFIG and AZURE_CONFIG["apiKey"]:
            return AzureOpenAI(
                azure_endpoint=AZURE_CONFIG["endpoint"],
                api_key=AZURE_CONFIG["apiKey"],
                api_version="2024-05-01-preview"
            )

        credential = DefaultAzureCredential(managed_identity_client_id=AZURE_CONFIG.get("managedIdentityClientId"))
        token_provider = lambda: credential.get_token("https://cognitiveservices.azure.com/.default").token
        return AzureOpenAI(
            azure_endpoint=AZURE_CONFIG["endpoint"],
            azure_ad_token_provider=token_provider,
            api_version="2024-05-01-preview"
        )
    except Exception as e:
        print(f"åˆå§‹åŒ– Azure Client å¤±è´¥äº†: {e}")
        return None

def get_date_str():
    return datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

def get_stock_reason(symbol, name, industry=None, first_time=None, client=None):
    """
    è·å–ä¸ªè‚¡ç›¸å…³æ–°é—»ï¼Œå¹¶å°è¯•åˆ©ç”¨ AI æ€»ç»“æ¶¨åœåŸå› 
    """
    try:
        # è·å–æœ€è¿‘çš„æ–°é—»
        news_df = ak.stock_news_em(symbol=symbol)
        if news_df.empty:
            return ""
        
        related_news = []
        news_contexts = [] # ç”¨äºå‘ç»™ AI çš„çº¯æ–‡æœ¬ç´ æ

        # éå†å‰ 10 æ¡æ–°é—»ï¼Œç­›é€‰æœ‰æ•ˆä¿¡æ¯ï¼Œä¿ç•™ 3 æ¡å±•ç¤º
        valid_count = 0 
        
        for _, row in news_df.head(10).iterrows():
            title = str(row.get('æ–°é—»æ ‡é¢˜', '')).strip()
            content = str(row.get('æ–°é—»å†…å®¹', '')).strip()
            
            if not title or title == 'nan':
                continue

            summary = content[:100].replace('\n', ' ').strip()
            
            # æ ¼å¼åŒ–å±•ç¤ºç”¨
            if valid_count < 3:
                news_item = f"    * èµ„è®¯: {title} (æ‘˜è¦: {summary}...)"
                related_news.append(news_item)
            
            # æ”¶é›†ç»™ AI åˆ†æç”¨ (å¤šæ”¶é›†ä¸€ç‚¹ä¹Ÿå¯ä»¥ï¼Œæ¯”å¦‚å‰5æ¡çš„æ ‡é¢˜)
            news_contexts.append(f"- {title}: {summary}")
            
            valid_count += 1
        
        analysis_result = ""
        # å¦‚æœæœ‰å®¢æˆ·ç«¯ä¸”æœ‰æ–°é—»ï¼Œè°ƒç”¨ AI è¿›è¡Œæ€»ç»“
        if client and news_contexts:
            try:
                # æ„é€ ä¸€ä¸ªå° Prompt
                context_str = "\n".join(news_contexts[:5]) # ç»™ AI çœ‹å‰5æ¡
                prompt = f"""è¯·é˜…è¯»ä»¥ä¸‹å…³äºè‚¡ç¥¨ã€{name}ã€‘(ä»£ç {symbol})çš„è¿‘æœŸæ–°é—»ï¼Œåˆ†æå¹¶æ€»ç»“å…¶æ¶¨åœæˆ–å¼‚åŠ¨çš„æ ¸å¿ƒåŸå› ã€‚
                
æ–°é—»åˆ—è¡¨ï¼š
{context_str}

è¦æ±‚ï¼š
1. é‡ç‚¹æŒ‡å‡ºå…·ä½“åˆ©å¥½äº‹ä»¶ã€æ¶‰åŠçš„å…·ä½“æ¦‚å¿µï¼ˆå¦‚â€œä½ç©ºç»æµâ€ã€â€œå¹¶è´­é‡ç»„â€ç­‰ï¼‰ã€‚
2. é€»è¾‘æ¸…æ™°ï¼Œä¸€å¥è¯æ¦‚æ‹¬ï¼Œä¸è¦æœ‰â€œç»åˆ†æâ€ç­‰åºŸè¯ã€‚
3. æ§åˆ¶åœ¨ 60 å­—ä»¥å†…ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚"""

                # å¿«é€Ÿè°ƒç”¨ï¼Œtemperature ä½ä¸€ç‚¹ä¿è¯ç¨³å®š
                response = client.chat.completions.create(
                    model=AZURE_CONFIG["deploymentName"],
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€åAè‚¡çŸ­çº¿åˆ†æå¸ˆã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=200
                )
                reason_text = response.choices[0].message.content.strip()
                # å»é™¤å¯èƒ½çš„å¼•å·
                reason_text = reason_text.replace('"', '').replace("'", "")
                
                # è¡¥å……æ¿å—å’Œæ—¶é—´ä¿¡æ¯
                meta_info = ""
                if industry:
                    meta_info += f"[æ¿å—:{industry}]"
                if first_time:
                    # æ ¼å¼åŒ–æ—¶é—´ HHMMSS -> HH:MM:SS
                    ft_str = str(first_time).strip()
                    if len(ft_str) == 6 and ft_str.isdigit():
                        ft_str = f"{ft_str[:2]}:{ft_str[2:4]}:{ft_str[4:]}"
                    meta_info += f"[é¦–å°:{ft_str}]"
                if meta_info:
                    meta_info += " "
                
                analysis_result = f"    * AIåˆ†æ: {meta_info}{reason_text}"
            except Exception as ai_e:
                # AI åˆ†æå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªæ‰“å°æ—¥å¿—
                print(f"  [AIåˆ†æ {name} å¤±è´¥]: {ai_e}")

        # ç»„åˆè¿”å›ç»“æœ
        result_parts = []
        if analysis_result:
            result_parts.append("\n" + analysis_result)
        if related_news:
             result_parts.append("\n" + "\n".join(related_news))
             
        return "".join(result_parts)

    except Exception:
        pass
    return ""

def fetch_daily_market_data():
    print("æ­£åœ¨ä» AkShare è·å–å®æ—¶å¸‚åœºæ•°æ® (æ—¥æŠ¥æ¨¡å¼)...")
    
    # å°è¯•åˆå§‹åŒ– AI å®¢æˆ·ç«¯ç”¨äºä¸ªè‚¡åˆ†æ
    ai_client = get_azure_client()
    if ai_client:
        print("DEBUG: AI è¾…åŠ©åˆ†ææ¨¡å—å·²å°±ç»ª")
    else:
        print("DEBUG: AI æ¨¡å—åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä»…å±•ç¤ºåŸå§‹æ–°é—»")

    data_summary = []
    
    # å®šä¹‰ç¼“å­˜æ–‡ä»¶è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cache_file = os.path.join(script_dir, "last_successful_data_daily.txt")

    # 1. è·å–ä¸»è¦æŒ‡æ•°è¡Œæƒ… (ä½¿ç”¨å†å²æ—¥çº¿æ•°æ®ä»¥ç¡®ä¿é€šè¿‡æ—¥æœŸåŒ¹é…)
    data_summary.append("ã€æ˜¨æ—¥å¸‚åœºè¡Œæƒ…ã€‘")
    try:
        # ä½¿ç”¨ ak.stock_zh_index_daily è·å–å†å²æ•°æ®
        # sh000001: ä¸Šè¯æŒ‡æ•°, sz399001: æ·±è¯æˆæŒ‡, sz399006: åˆ›ä¸šæ¿æŒ‡
        target_indices = {
            'sh000001': 'ä¸Šè¯æŒ‡æ•°',
            'sz399001': 'æ·±è¯æˆæŒ‡',
            'sz399006': 'åˆ›ä¸šæ¿æŒ‡'
        }
        
        # ç¡®å®šâ€œæ˜¨æ—¥â€çš„æ—¥æœŸï¼šå³æœ€è¿‘ä¸€ä¸ªå·²æ”¶ç›˜çš„äº¤æ˜“æ—¥
        # ç­–ç•¥ï¼šè·å–ä¸Šè¯æŒ‡æ•°æœ€è¿‘å‡ è¡Œæ•°æ®ï¼Œå–æœ€æ–°çš„ä¸€è¡Œä½œä¸ºå‚è€ƒæ—¥æœŸï¼ˆå¦‚æœä¸ç®—ä»Šæ—¥çš„è¯ï¼‰
        # å¦‚æœè„šæœ¬æ˜¯åœ¨äº¤æ˜“æ—¥ç›˜ä¸­è¿è¡Œï¼Œæˆ‘ä»¬å¯èƒ½æƒ³è¦çš„æ˜¯å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
        # ä½†é€šå¸¸æ™¨è®¯æ˜¯åœ¨ç¬¬äºŒå¤©æ—©ä¸Šè¿è¡Œï¼Œæ‰€ä»¥æƒ³è¦çš„æ˜¯ df.iloc[-1] (å¦‚æœ API è¿˜æ²¡æ›´æ–°ä»Šæ—¥æ•°æ®)
        # æˆ–è€… df.iloc[-2] (å¦‚æœ API å·²ç»æ›´æ–°äº†ä»Šæ—¥æ•°æ®)
        
        # å…ˆè·å–ä¸€æ¬¡ä¸Šè¯æŒ‡æ•°æ¥ç¡®å®šæ—¥æœŸ
        ref_df = ak.stock_zh_index_daily(symbol='sh000001')
        if ref_df.empty:
            raise Exception("æ— æ³•è·å–åŸºå‡†æŒ‡æ•°æ•°æ®")
            
        # ç®€å•çš„é€»è¾‘ï¼šå¦‚æœæœ€åä¸€è¡Œæ—¥æœŸæ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨ä¸æ˜¯æ™šä¸Šï¼Œé‚£å¯èƒ½æ˜¯ç›˜ä¸­æ•°æ®ï¼Œæˆ–è€…æˆ‘ä»¬å…¶å®æƒ³è¦æ˜¨å¤©çš„æ•°æ®
        # æ—¢ç„¶æ˜¯â€œæ™¨è®¯â€ï¼Œé€šå¸¸æ˜¯è¿˜æ²¡å¼€ç›˜æˆ–è€…åˆšå¼€ç›˜ï¼Œæ‰€ä»¥æˆ‘ä»¬æƒ³è¦çš„æ˜¯â€œæœ€è¿‘ä¸€ä¸ªå®Œæ•´çš„äº¤æ˜“æ—¥â€
        # æŸ¥çœ‹æœ€åä¸€è¡Œæ—¥æœŸ
        last_date = pd.to_datetime(ref_df.iloc[-1]['date']).date()
        current_date = datetime.datetime.now().date()
        
        target_row_idx = -1
        if last_date == current_date:
            # å¦‚æœAPIè¿”å›äº†ä»Šå¤©çš„æ•°æ®ï¼ˆè¯´æ˜ä»Šå¤©å·²ç»å¼€å§‹äº¤æ˜“æˆ–å·²ç»“æŸï¼‰ï¼Œä½œä¸ºæ™¨è®¯æˆ‘ä»¬åº”è¯¥å–æ˜¨å¤©ï¼ˆå³ä¸Šä¸€è¡Œï¼‰
            target_row_idx = -2
            
        report_date = ref_df.iloc[target_row_idx]['date']
        print(f"DEBUG: é€‰å®šçš„æŠ¥å‘Šæ—¥æœŸä¸º {report_date}")
        
        for code, name in target_indices.items():
            df = ak.stock_zh_index_daily(symbol=code)
            # æ‰¾åˆ°å¯¹åº”æ—¥æœŸçš„è¡Œ
            row = df[df['date'].astype(str) == str(report_date)]
            
            if not row.empty:
                close_price = row.iloc[0]['close']
                # è®¡ç®—æ¶¨è·Œå¹…: (close - prev_close) / prev_close
                # éœ€æ‰¾åˆ°å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
                row_idx = row.index[0]
                if row_idx > 0:
                    prev_close = df.iloc[row_idx - 1]['close']
                    change_pct = (close_price - prev_close) / prev_close * 100
                    
                    # æ ¼å¼åŒ–
                    change_pct_str = f"{change_pct:.2f}"
                    data_summary.append(f"{name}: {close_price:.2f} ({change_pct_str}%)")
                else:
                    data_summary.append(f"{name}: {close_price:.2f} (æ— æ³•è®¡ç®—æ¶¨è·Œ)")
            else:
                data_summary.append(f"{name}: æ•°æ®ç¼ºå¤±")

        data_summary.append("")
        print(f"DEBUG: æˆåŠŸè·å–å†å²æŒ‡æ•°æ•°æ®: {data_summary}")

    except Exception as e:
        print(f"è·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")

    # 1.2 è·å–ä¸¤å¸‚æˆäº¤é¢
    try:
        # ä½¿ç”¨ daily_em æ¥å£è·å–å†å²æˆäº¤é¢
        sh_daily = ak.stock_zh_index_daily_em(symbol='sh000001')
        sz_daily = ak.stock_zh_index_daily_em(symbol='sz399001')
        
        if not sh_daily.empty and not sz_daily.empty:
            # ç¡®ä¿æ—¥æœŸåˆ—ä¸º datetime ç±»å‹ä»¥ä¾¿åˆå¹¶
            sh_daily['date'] = pd.to_datetime(sh_daily['date'])
            sz_daily['date'] = pd.to_datetime(sz_daily['date'])
            
            # åˆå¹¶æ•°æ®
            merged = pd.merge(sh_daily[['date', 'amount']], sz_daily[['date', 'amount']], on='date', suffixes=('_sh', '_sz'))
            merged['total_amount'] = merged['amount_sh'] + merged['amount_sz']
            
            # åŒæ ·ä½¿ç”¨ report_date æ¥å®šä½æ•°æ®
            # æ³¨æ„ report_date æ˜¯ string æˆ– dateï¼Œéœ€è¦åŒ¹é…
            target_row = merged[merged['date'].dt.date.astype(str) == str(report_date)]
            
            if not target_row.empty:
                today_row = target_row.iloc[0]
                # è·å–å‰ä¸€äº¤æ˜“æ—¥
                # åœ¨ merged ä¸­æ‰¾åˆ° target_row çš„å‰ä¸€è¡Œ
                # è¿™ç§æ–¹æ³•ä¾èµ– merged æ˜¯æŒ‰æ—¶é—´æ’åºçš„ï¼ˆé€šå¸¸æ˜¯ï¼‰
                target_idx = target_row.index[0]
                
                today_amount = today_row['total_amount']
                today_trillion = today_amount / 1e12
                
                data_summary.append(f"ã€æ˜¨æ—¥æˆäº¤é¢ã€‘")
                data_summary.append(f"æ²ªæ·±ä¸¤å¸‚æ€»æˆäº¤é¢: {today_trillion:.2f}ä¸‡äº¿å…ƒ")
                
                if target_idx > 0:
                    prev_row = merged.iloc[target_idx - 1]
                    prev_amount = prev_row['total_amount']
                    change = today_amount - prev_amount
                    change_trillion = change / 1e12
                    
                    if change > 0:
                        data_summary.append(f"è¾ƒå‰ä¸€äº¤æ˜“æ—¥æ”¾é‡: {abs(change_trillion):.2f}ä¸‡äº¿å…ƒ")
                    else:
                        data_summary.append(f"è¾ƒå‰ä¸€äº¤æ˜“æ—¥ç¼©é‡: {abs(change_trillion):.2f}ä¸‡äº¿å…ƒ")
                
                data_summary.append(f"- ä¸Šè¯æŒ‡æ•°æˆäº¤é¢: {today_row['amount_sh']/1e12:.2f}ä¸‡äº¿å…ƒ")
                data_summary.append(f"- æ·±è¯æˆæŒ‡æˆäº¤é¢: {today_row['amount_sz']/1e12:.2f}ä¸‡äº¿å…ƒ")
                data_summary.append("")
                print(f"DEBUG: æˆåŠŸè®¡ç®—æˆäº¤é¢: {today_trillion:.2f}ä¸‡äº¿")
            else:
                print(f"æœªæ‰¾åˆ°æ—¥æœŸ {report_date} çš„æˆäº¤é¢æ•°æ®")
            
    except Exception as e:
        print(f"è®¡ç®—æˆäº¤é¢å¤±è´¥: {e}")

    # 1.5 è·å–æ¿å—æ¶¨è·Œå¹… (æ–°å¢)
    try:
        data_summary.append("ã€æ˜¨æ—¥æ¿å—è¡¨ç°ã€‘")
        # è¡Œä¸šæ¿å—
        ind_df = ak.stock_board_industry_name_em()
        if not ind_df.empty and 'æ¶¨è·Œå¹…' in ind_df.columns:
            ind_df['æ¶¨è·Œå¹…'] = pd.to_numeric(ind_df['æ¶¨è·Œå¹…'], errors='coerce')
            ind_sorted = ind_df.sort_values(by='æ¶¨è·Œå¹…', ascending=False)
            
            top_ind = ind_sorted.head(5)
            bottom_ind = ind_sorted.tail(5)
            
            data_summary.append("é¢†æ¶¨è¡Œä¸š:")
            for _, row in top_ind.iterrows():
                data_summary.append(f"- {row['æ¿å—åç§°']}: {row['æ¶¨è·Œå¹…']:.2f}% (é¢†æ¶¨è‚¡: {row.get('é¢†æ¶¨è‚¡ç¥¨', 'N/A')})")
            
            data_summary.append("é¢†è·Œè¡Œä¸š:")
            for _, row in bottom_ind.iterrows():
                data_summary.append(f"- {row['æ¿å—åç§°']}: {row['æ¶¨è·Œå¹…']:.2f}% (é¢†è·Œè‚¡: {row.get('é¢†æ¶¨è‚¡ç¥¨', 'N/A')})")
        
        # æ¦‚å¿µæ¿å—
        con_df = ak.stock_board_concept_name_em()
        if not con_df.empty and 'æ¶¨è·Œå¹…' in con_df.columns:
            con_df['æ¶¨è·Œå¹…'] = pd.to_numeric(con_df['æ¶¨è·Œå¹…'], errors='coerce')
            con_sorted = con_df.sort_values(by='æ¶¨è·Œå¹…', ascending=False)
            
            data_summary.append("é¢†æ¶¨æ¦‚å¿µ:")
            for _, row in con_sorted.head(5).iterrows():
                data_summary.append(f"- {row['æ¿å—åç§°']}: {row['æ¶¨è·Œå¹…']:.2f}%")
        
        data_summary.append("")
    except Exception as e:
        print(f"è·å–æ¿å—æ•°æ®å¤±è´¥: {e}")

    # 2. è·å–è´¢ç»æ–°é—» (è´¢è”ç¤¾ç”µæŠ¥)
    try:
        # stock_info_global_cls è´¢è”ç¤¾ç”µæŠ¥
        # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•° 'days'
        news_df = ak.stock_info_global_cls()
        
        data_summary.append("ã€æ˜¨æ—¥èµ„è®¯ã€‘")
        if not news_df.empty:
            print(f"DEBUG: æˆåŠŸè·å–åˆ° {len(news_df)} æ¡æ–°é—»ã€‚")
            first_title = news_df.iloc[0].get('title') or news_df.iloc[0].get('æ ‡é¢˜')
            first_time = news_df.iloc[0].get('time') or news_df.iloc[0].get('å‘å¸ƒæ—¶é—´')
            print(f"DEBUG: æœ€æ–°ä¸€æ¡æ–°é—»: [{first_time}] {first_title}")

            # ç¡®ä¿æŒ‰æ—¶é—´æ’åº (å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¶é—´æˆ–å‘å¸ƒæ—¶é—´)
            # news_df = news_df.sort_values(by='time', ascending=False) 
            
            # å–å‰ 50 æ¡ (å¢åŠ æ•°é‡)
            for _, row in news_df.head(50).iterrows():
                # é€‚é…ä¸­æ–‡åˆ—å
                title = row.get('title') or row.get('æ ‡é¢˜') or ''
                content = row.get('content') or row.get('å†…å®¹') or ''
                
                if title:
                    data_summary.append(f"- {title}")
                elif content:
                    data_summary.append(f"- {content[:100]}...")
    except Exception as e:
        print(f"è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")

    # 3. è·å–æ¶¨åœè‚¡æ±  (æ–°å¢)
    try:
        zt_pool_df = None
        # å°è¯•è·å–æœ€è¿‘ 5 å¤©çš„æ•°æ® (æ‰¾åˆ°æœ€è¿‘çš„ä¸€ä¸ªäº¤æ˜“æ—¥)
        for delta in range(0, 5):
            target_date = (datetime.datetime.now() - datetime.timedelta(days=delta)).strftime("%Y%m%d")
            try:
                df = ak.stock_zt_pool_em(date=target_date)
                if not df.empty:
                    zt_pool_df = df
                    print(f"DEBUG: æˆåŠŸè·å–åˆ° {target_date} çš„æ¶¨åœæ•°æ®ï¼Œå…± {len(df)} æ¡ã€‚")
                    break
            except:
                continue
        
        data_summary.append("ã€æ˜¨æ—¥æ¶¨åœæ¢¯é˜Ÿæ•°æ®ã€‘")
        if zt_pool_df is not None and not zt_pool_df.empty:
            # ç¡®ä¿è¿æ¿æ•°æ˜¯æ•°å­—
            if 'è¿æ¿æ•°' in zt_pool_df.columns:
                zt_pool_df['è¿æ¿æ•°'] = pd.to_numeric(zt_pool_df['è¿æ¿æ•°'], errors='coerce')
                zt_pool_df = zt_pool_df.sort_values(by='è¿æ¿æ•°', ascending=False)
            
            # é™åˆ¶ Prompt ä¸­å±•ç¤ºçš„è‚¡ç¥¨æ€»æ•°ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡º
            # MAX_DISPLAY = 50 # ç§»é™¤é™åˆ¶ï¼Œå±•ç¤ºå…¨éƒ¨
            display_count = 0
            
            # åˆ†æè®¡æ•°å™¨
            analyzed_count = 0
            
            for _, row in zt_pool_df.iterrows():
                lb = row.get('è¿æ¿æ•°')
                # åˆ¤å®šæ˜¯å¦ä¸ºé«˜æ ‡è‚¡ (>=2è¿æ¿)
                is_high_lb = isinstance(lb, (int, float)) and lb >= 2
                
                # ç§»é™¤æˆªæ–­é€»è¾‘ï¼Œç¡®ä¿å±•ç¤ºæ‰€æœ‰è‚¡ç¥¨
                # if display_count >= MAX_DISPLAY and not is_high_lb:
                #      remaining = len(zt_pool_df) - display_count
                #      data_summary.append(f"... (å‰©ä½™ {remaining} åªé¦–æ¿/ä½ä½æ¶¨åœè‚¡ç•¥å»)")
                #      break

                name = row.get('åç§°')
                code = row.get('ä»£ç ')
                first_time = row.get('é¦–æ¬¡å°æ¿æ—¶é—´')
                last_time = row.get('æœ€åå°æ¿æ—¶é—´')
                open_times = row.get('ç‚¸æ¿æ¬¡æ•°')
                industry = row.get('æ‰€å±è¡Œä¸š')
                
                reason_str = ""
                
                # --- æ ¸å¿ƒåˆ†æé€»è¾‘ ---
                # 1. å¦‚æœæ˜¯ 2è¿æ¿åŠä»¥ä¸Šï¼šå¿…é¡»åˆ†æ
                # 2. å¦‚æœæ˜¯ é¦–æ¿ï¼šåªæœ‰åœ¨"å·²åˆ†ææ€»æ•°"ä¸è¶³ 10 ä¸ªæ—¶æ‰åˆ†æ (å¡«è¡¥ç©ºä½)
                should_analyze = is_high_lb or (analyzed_count < 10)
                
                if should_analyze:
                    print(f"DEBUG: æ­£åœ¨è·å– {name} ({code}) çš„æ¶¨åœåŸå› ...")
                    reason_str = get_stock_reason(code, name, industry=industry, first_time=first_time, client=ai_client)
                    analyzed_count += 1
                
                # æ„é€ æè¿°
                data_summary.append(f"- {name} ({lb}è¿æ¿): è¡Œä¸š-{industry}, é¦–æ¬¡å°æ¿-{first_time}, æœ€åå°æ¿-{last_time}, ç‚¸æ¿-{open_times}æ¬¡{reason_str}")
                display_count += 1
        else:
            data_summary.append("æœªè·å–åˆ°æ¶¨åœæ•°æ®ã€‚")

    except Exception as e:
        print(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")
        
    final_text = "\n".join(data_summary)
    
    # ç®€å•çš„æœ‰æ•ˆæ€§æ£€æŸ¥ï¼šå¦‚æœå†…å®¹å¤ªçŸ­æˆ–ç¼ºå°‘å…³é”®æ¿å—ï¼Œè§†ä¸ºè·å–å¤±è´¥
    is_valid = len(final_text) > 100 and "ã€æ˜¨æ—¥å¸‚åœºè¡Œæƒ…ã€‘" in final_text
    
    if is_valid:
        # è·å–æˆåŠŸï¼Œä¿å­˜åˆ°ç¼“å­˜
        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                f.write(final_text)
            print(f"DEBUG: æœ€æ–°æ•°æ®å·²æˆåŠŸå¤‡ä»½è‡³ {cache_file}")
        except Exception as e:
            print(f"æ•°æ®å¤‡ä»½å¤±è´¥: {e}")
    else:
        # è·å–å¤±è´¥ï¼Œå°è¯•è¯»å–ç¼“å­˜
        print("âš ï¸ è­¦å‘Š: æœ¬æ¬¡è‡ªåŠ¨è·å–çš„æ•°æ®ä¼¼ä¹ä¸å®Œæ•´æˆ–ä¸ºç©ºã€‚")
        if os.path.exists(cache_file):
            print("ğŸ”„ æ­£åœ¨å°è¯•åŠ è½½ä¸Šæ¬¡æˆåŠŸçš„å¤‡ä»½æ•°æ®...")
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cached_text = f.read()
                if len(cached_text) > 50:
                    final_text = cached_text + "\n\n(æ³¨ï¼šä»¥ä¸Šä¸ºå†å²å¤‡ä»½æ•°æ®ï¼Œå› å®æ—¶è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§)"
                    print("âœ… æˆåŠŸåŠ è½½å¤‡ä»½æ•°æ®ã€‚")
            except Exception as e:
                print(f"åŠ è½½å¤‡ä»½æ•°æ®å¤±è´¥: {e}")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å¤‡ä»½æ•°æ®ã€‚")

    return final_text

def fetch_weekly_market_data():
    print("æ­£åœ¨è·å–å‘¨æŠ¥æ•°æ® (è¿‡å»5ä¸ªäº¤æ˜“æ—¥)...")
    
    # åˆå§‹åŒ– AI å®¢æˆ·ç«¯
    ai_client = get_azure_client()
    if ai_client:
        print("DEBUG: AI è¾…åŠ©åˆ†ææ¨¡å—å·²å°±ç»ª (Weekly)")
        
    data_summary = []
    
    # å®šä¹‰ç¼“å­˜æ–‡ä»¶è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cache_file = os.path.join(script_dir, "last_successful_data_weekly.txt")

    # 1. æŒ‡æ•°å‘¨åº¦è¡¨ç°
    try:
        # ä¸Šè¯æŒ‡æ•°, æ·±è¯æˆæŒ‡, åˆ›ä¸šæ¿æŒ‡
        indices = {
            'sh000001': 'ä¸Šè¯æŒ‡æ•°',
            'sz399001': 'æ·±è¯æˆæŒ‡',
            'sz399006': 'åˆ›ä¸šæ¿æŒ‡'
        }
        data_summary.append("ã€ä¸Šå‘¨æŒ‡æ•°è¡¨ç°ã€‘")
        
        for code, name in indices.items():
            # è·å–æ—¥çº¿æ•°æ®
            df = ak.stock_zh_index_daily(symbol=code)
            if not df.empty and len(df) >= 5:
                last_5 = df.tail(5)
                start_close = last_5.iloc[0]['close'] 
                end_close = last_5.iloc[-1]['close']
                pct_change = (end_close - start_close) / start_close * 100
                
                data_summary.append(f"{name}: ä¸Šå‘¨æ”¶ç›˜ {end_close:.2f}, 5æ—¥æ¶¨è·Œå¹… {pct_change:.2f}%")
            else:
                data_summary.append(f"{name}: æ•°æ®ä¸è¶³")
        data_summary.append("")
    except Exception as e:
        print(f"è·å–æŒ‡æ•°å‘¨åº¦æ•°æ®å¤±è´¥: {e}")

    # 1.2 Calculate Weekly Turnover (New)
    try:
        # Use EM interface for daily data as it includes 'amount'
        sh_df = ak.stock_zh_index_daily_em(symbol='sh000001')
        sz_df = ak.stock_zh_index_daily_em(symbol='sz399001')
        
        if not sh_df.empty and not sz_df.empty:
            # Ensure date column is datetime for merging
            sh_df['date'] = pd.to_datetime(sh_df['date'])
            sz_df['date'] = pd.to_datetime(sz_df['date'])
            
            # Merge on date
            merged = pd.merge(sh_df[['date', 'amount']], sz_df[['date', 'amount']], on='date', suffixes=('_sh', '_sz'))
            
            # Get last 5 days
            last_5 = merged.tail(5)
            
            if not last_5.empty:
                last_5['total_amount'] = last_5['amount_sh'] + last_5['amount_sz']
                avg_turnover = last_5['total_amount'].mean()
                avg_turnover_trillion = avg_turnover / 1e12
                
                data_summary.append(f"ã€ä¸Šå‘¨æˆäº¤é¢ã€‘")
                data_summary.append(f"æ²ªæ·±ä¸¤å¸‚æ—¥å‡æˆäº¤é¢: {avg_turnover_trillion:.2f}ä¸‡äº¿å…ƒ")
                data_summary.append("")
                print(f"DEBUG: æˆåŠŸè®¡ç®—å‘¨å‡æˆäº¤é¢: {avg_turnover_trillion:.2f}ä¸‡äº¿")
            
    except Exception as e:
        print(f"è®¡ç®—å‘¨æˆäº¤é¢å¤±è´¥: {e}")

    # 1.5 è·å–æ¿å—æ¶¨è·Œå¹… (å‘¨åº¦ - ä½¿ç”¨å½“å‰å®æ—¶æ’åè¿‘ä¼¼)
    try:
        data_summary.append("ã€ä¸Šå‘¨æ¿å—è¡¨ç° (å‚è€ƒ)ã€‘")
        # è¡Œä¸šæ¿å—
        ind_df = ak.stock_board_industry_name_em()
        if not ind_df.empty and 'æ¶¨è·Œå¹…' in ind_df.columns:
            ind_df['æ¶¨è·Œå¹…'] = pd.to_numeric(ind_df['æ¶¨è·Œå¹…'], errors='coerce')
            ind_sorted = ind_df.sort_values(by='æ¶¨è·Œå¹…', ascending=False)
            
            top_ind = ind_sorted.head(5)
            bottom_ind = ind_sorted.tail(5)
            
            data_summary.append("é¢†æ¶¨è¡Œä¸š:")
            for _, row in top_ind.iterrows():
                data_summary.append(f"- {row['æ¿å—åç§°']}: {row['æ¶¨è·Œå¹…']:.2f}%")
            
            data_summary.append("é¢†è·Œè¡Œä¸š:")
            for _, row in bottom_ind.iterrows():
                data_summary.append(f"- {row['æ¿å—åç§°']}: {row['æ¶¨è·Œå¹…']:.2f}%")
        data_summary.append("")
    except Exception as e:
        print(f"è·å–æ¿å—æ•°æ®å¤±è´¥: {e}")

    # 2. æ¶¨åœæ¢¯é˜Ÿ (å‘¨åº¦æ±‡æ€»)
    try:
        data_summary.append("ã€ä¸Šå‘¨å¼ºåŠ¿è‚¡ (æ¶¨åœç»Ÿè®¡)ã€‘")
        zt_counts = {}
        
        successful_days = 0
        check_days = 10
        current_date = datetime.datetime.now()
        
        for i in range(check_days):
            if successful_days >= 5:
                break
            
            target_date = (current_date - datetime.timedelta(days=i)).strftime("%Y%m%d")
            try:
                df = ak.stock_zt_pool_em(date=target_date)
                if not df.empty:
                    successful_days += 1
                    print(f"DEBUG: è·å–åˆ° {target_date} æ¶¨åœæ•°æ®")
                    for _, row in df.iterrows():
                        name = row['åç§°']
                        industry = row['æ‰€å±è¡Œä¸š']
                        code = row['ä»£ç ']
                        if name not in zt_counts:
                            zt_counts[name] = {'count': 0, 'industry': industry, 'code': code}
                        zt_counts[name]['count'] += 1
            except:
                pass
        
        # Sort by count
        sorted_zt = sorted(zt_counts.items(), key=lambda x: x[1]['count'], reverse=True)
        
        # All
        for name, info in sorted_zt:
            reason_str = ""
            if info['count'] >= 2:
                print(f"DEBUG: æ­£åœ¨è·å– {name} ({info['code']}) çš„å‘¨åº¦æ¶¨åœåŸå› ...")
                reason_str = get_stock_reason(info['code'], name, industry=info['industry'], client=ai_client)

            data_summary.append(f"- {name} ({info['industry']}): ä¸Šå‘¨æ¶¨åœ {info['count']} æ¬¡{reason_str}")
            
    except Exception as e:
        print(f"è·å–å‘¨åº¦æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    # 3. News (Just fetch recent)
    try:
        news_df = ak.stock_info_global_cls()
        data_summary.append("ã€è¿‘æœŸé‡è¦èµ„è®¯ã€‘")
        if not news_df.empty:
             # å–å‰ 50 æ¡
             for _, row in news_df.head(50).iterrows():
                title = row.get('title') or row.get('æ ‡é¢˜') or ''
                if title:
                    data_summary.append(f"- {title}")
    except Exception as e:
        print(f"è·å–æ–°é—»å¤±è´¥: {e}")
        
    final_text = "\n".join(data_summary)
    
    # ç®€å•çš„æœ‰æ•ˆæ€§æ£€æŸ¥
    is_valid = len(final_text) > 100 and "ã€ä¸Šå‘¨æŒ‡æ•°è¡¨ç°ã€‘" in final_text
    
    if is_valid:
        # è·å–æˆåŠŸï¼Œä¿å­˜åˆ°ç¼“å­˜
        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                f.write(final_text)
            print(f"DEBUG: æœ€æ–°å‘¨æŠ¥æ•°æ®å·²æˆåŠŸå¤‡ä»½è‡³ {cache_file}")
        except Exception as e:
            print(f"æ•°æ®å¤‡ä»½å¤±è´¥: {e}")
    else:
        # è·å–å¤±è´¥ï¼Œå°è¯•è¯»å–ç¼“å­˜
        print("âš ï¸ è­¦å‘Š: æœ¬æ¬¡è‡ªåŠ¨è·å–çš„å‘¨æŠ¥æ•°æ®ä¼¼ä¹ä¸å®Œæ•´æˆ–ä¸ºç©ºã€‚")
        if os.path.exists(cache_file):
            print("ğŸ”„ æ­£åœ¨å°è¯•åŠ è½½ä¸Šæ¬¡æˆåŠŸçš„å‘¨æŠ¥å¤‡ä»½æ•°æ®...")
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cached_text = f.read()
                if len(cached_text) > 50:
                    final_text = cached_text + "\n\n(æ³¨ï¼šä»¥ä¸Šä¸ºå†å²å¤‡ä»½æ•°æ®ï¼Œå› å®æ—¶è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§)"
                    print("âœ… æˆåŠŸåŠ è½½å¤‡ä»½æ•°æ®ã€‚")
            except Exception as e:
                print(f"åŠ è½½å¤‡ä»½æ•°æ®å¤±è´¥: {e}")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å¤‡ä»½æ•°æ®ã€‚")

    return final_text

def read_news_input(file_path="news_input.txt"):
    if not os.path.exists(file_path):
        return None
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

def generate_briefing(news_content, report_type="daily"):
    # ä½¿ç”¨ Managed Identity è·å–å‡­è¯ (å¤ç”¨ get_azure_client çš„é€»è¾‘ï¼Œæˆ–è€…ç›´æ¥è°ƒç”¨)
    client = get_azure_client()
    if not client:
        return "æ— æ³•åˆå§‹åŒ– Azure OpenAI å®¢æˆ·ç«¯"
    
    date_str = get_date_str()
    date_str_header = datetime.datetime.now().strftime("%Y%m%d")
    
    if report_type == "weekly":
        title_text = "ç¿ç»„åˆå°çº¢èŠ±å‘¨æŠ¥"
        prompt_role = "ä¸Šå‘¨"
        prompt_style = "ç¿ç»„åˆå°çº¢èŠ±å‘¨æŠ¥"
        section1_title = "ä¸Šå‘¨å¸‚åœºå…¨æ™¯å›é¡¾"
        section1_desc = "æ€»ç»“ä¸Šå‘¨æŒ‡æ•°æ•´ä½“è¡¨ç°ï¼ˆæ¶¨è·Œå¹…ï¼‰ï¼Œæˆäº¤é‡å˜åŒ–è¶‹åŠ¿ã€‚æ¦‚æ‹¬ä¸Šå‘¨çš„ä¸»çº¿è¡Œæƒ…å’Œé£æ ¼åˆ‡æ¢ã€‚è¯·åŠ¡å¿…ä½¿ç”¨â€œä¸Šå‘¨â€ä½œä¸ºæ—¶é—´çŠ¶è¯­ã€‚"
        section2_title = "æ·±åº¦é€»è¾‘ä¸å¤ç›˜"
        section2_desc = "åˆ†æä¸Šå‘¨æ¶¨è·Œçš„æ ¸å¿ƒé€»è¾‘ã€‚ç‚¹è¯„ä¸Šå‘¨è¡¨ç°æœ€å¼ºçš„æ¿å—å’Œä¸ªè‚¡ï¼ˆç»“åˆæ¶¨åœç»Ÿè®¡ï¼‰ã€‚"
        section3_title = "çƒ­ç‚¹é¢˜æä¸æ–°é—»é©±åŠ¨"
        section3_desc = "å°†ã€è¿‘æœŸé‡è¦èµ„è®¯ã€‘èå…¥åˆ°æ¿å—åˆ†æä¸­ã€‚é‡ç‚¹æŒ–æ˜ç§‘æŠ€ã€æ¶ˆè´¹ã€æ”¿ç­–ç›¸å…³çš„é¢˜æã€‚å†…å®¹è¦ä¸°å¯Œè¯¦å®ï¼Œä¸è¦ç®€ç•¥ã€‚"
        section4_title = "åå¸‚å±•æœ›ä¸ç­–ç•¥å»ºè®®"
        section4_desc = "ç»™å‡ºå¯¹æœ¬å‘¨å¸‚åœºçš„åˆ¤æ–­ã€‚ç»™å‡ºå…·ä½“æ“ä½œå»ºè®®ã€‚"
    else:
        title_text = "ç¿ç»„åˆå°çº¢èŠ±æ™¨è®¯"
        prompt_role = "æ˜¨æ—¥"
        prompt_style = "ç¿ç»„åˆå°çº¢èŠ±æ™¨è®¯"
        section1_title = "å¸‚åœºå…¨æ™¯å›é¡¾"
        section1_desc = "æè¿°æ˜¨æ—¥æŒ‡æ•°è¡¨ç°ï¼ˆæ¶¨è·Œå¹…ï¼‰ã€æˆäº¤é‡‘é¢åŠå˜åŒ–ï¼ˆå¿…é¡»æåŠå…·ä½“æ•°å€¼ï¼‰ã€å¸‚åœºæƒ…ç»ªï¼ˆå¦‚â€œæ™®æ¶¨â€ã€â€œåˆ†åŒ–â€ã€â€œä¿®å¤â€ï¼‰ã€‚æ¦‚æ‹¬é¢†æ¶¨å’Œé¢†è·Œçš„æ¿å—ã€‚è¯·åŠ¡å¿…ä½¿ç”¨â€œæ˜¨æ—¥â€ä½œä¸ºæ—¶é—´çŠ¶è¯­ã€‚"
        section2_title = "å¸‚åœºæ·±åº¦é€»è¾‘åˆ†æ"
        section2_desc = "åˆ†ææ¶¨è·ŒèƒŒåçš„åŸå› ï¼ˆå¦‚â€œæ”¿ç­–é©±åŠ¨â€ã€â€œå¤–å›´å½±å“â€ã€â€œèµ„é‡‘é«˜ä½åˆ‡æ¢â€ï¼‰ã€‚ç‚¹è¯„å¸‚åœºé£æ ¼ï¼ˆå¦‚â€œæƒé‡æ­å°â€ã€â€œé¢˜æå”±æˆâ€ã€â€œé«˜ä½è‚¡åˆ†æ­§â€ï¼‰ã€‚ç»“åˆã€æ¶¨åœæ¢¯é˜Ÿæ•°æ®ã€‘åŠå…¶ä¸­çš„ä¸ªè‚¡æ¶¨åœåŸå› ï¼ˆå¦‚æœ‰ï¼‰ï¼Œç‚¹è¯„è¿æ¿é«˜åº¦å’ŒçŸ­çº¿æƒ…ç»ªã€‚"
        section3_title = "çƒ­ç‚¹é¢˜æä¸æ–°é—»é©±åŠ¨"
        section3_desc = "å°†ã€æœ€æ–°èµ„è®¯ã€‘ä¸­çš„æ–°é—»èå…¥åˆ°æ¿å—åˆ†æä¸­ã€‚ä¸è¦ç½—åˆ—æ–°é—»ï¼Œè€Œæ˜¯å†™æˆâ€œå—...æ¶ˆæ¯åˆºæ¿€ï¼Œ...æ¿å—è¡¨ç°æ´»è·ƒâ€ã€‚å†…å®¹è¦ä¸°å¯Œè¯¦å®ï¼Œä¸è¦ç®€ç•¥ã€‚"
        section4_title = "åå¸‚å±•æœ›ä¸ç­–ç•¥å»ºè®®"
        section4_desc = "ç»™å‡ºå¯¹ä»Šæ—¥æˆ–çŸ­æœŸå¸‚åœºçš„åˆ¤æ–­ã€‚ç»™å‡ºå…·ä½“æ“ä½œå»ºè®®ã€‚"

    # å®šä¹‰æ™¨æŠ¥/å‘¨æŠ¥çš„æ ·å¼æ¨¡æ¿
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¹¿å‘è¯åˆ¸æŠ•èµ„é¡¾é—®ï¼ˆxxxï¼Œæ‰§ä¸šè¯ä¹¦ xxxxxxï¼‰ã€‚è¯·æ ¹æ®æä¾›çš„{prompt_role}å¸‚åœºèµ„è®¯ï¼Œæ’°å†™ä¸€ç¯‡é£æ ¼ä¸¥æ ¼æ¨¡ä»¿â€œ{prompt_style}â€çš„æŠ•èµ„æŠ¥å‘Šã€‚

    ### 1. æ ¸å¿ƒæ ·å¼è§„åˆ™ (HTML in Markdown)
    è¯·ç›´æ¥è¾“å‡ºåŒ…å« HTML æ ‡ç­¾çš„ Markdownï¼Œä»¥å®ç°å¤æ‚çš„æ’ç‰ˆå’Œé¢œè‰²ã€‚
    **ä¸¥ç¦ä½¿ç”¨ Markdown çš„åˆ—è¡¨ï¼ˆå¦‚ - æˆ– 1.ï¼‰æˆ–åˆ†æ®µæ ‡é¢˜ï¼ˆå¦‚ ## æ ‡é¢˜ï¼‰ï¼Œæ‰€æœ‰å†…å®¹å¿…é¡»æ˜¯ 3-4 æ®µç´§å‡‘çš„æ®µè½æ–‡æœ¬ï¼Œåƒæ–°é—»é€šç¨¿ä¸€æ ·è¿è´¯ã€‚**

    **å¤´éƒ¨æ’ç‰ˆ (å¿…é¡»å®Œå…¨ä¸€è‡´):**
    è¯·ä½¿ç”¨ HTML è¡¨æ ¼æ¥æ¨¡æ‹Ÿå¤´éƒ¨å¸ƒå±€ï¼š
    ```html
    <table style="width: 100%; border: none; margin-bottom: 10px;">
        <tr>
            <td style="text-align: left; width: 60%; vertical-align: bottom;">
                <span style="color: red; font-size: 24px; font-weight: bold;">{title_text}</span>
            </td>
            <td style="text-align: right; width: 40%; vertical-align: bottom;">
                <span style="color: red; font-weight: bold; font-size: 12px;">ç»„åˆå»ºè®®ä»…ä¾›å‚è€ƒï¼Œè‚¡å¸‚æœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚</span>
            </td>
        </tr>
        <tr>
            <td style="text-align: left;">
                <span style="color: blue; font-weight: bold; text-decoration: underline;">Will.S å¹¿å‘è¯åˆ¸æŠ•èµ„é¡¾é—® (S0260617110030)</span>
            </td>
            <td style="text-align: right;">
                <span style="border: 1px solid black; padding: 2px; font-weight: bold;">{date_str_header}</span>
            </td>
        </tr>
    </table>
    <hr style="border-top: 2px solid black; margin-top: 0px;">
    ```

    ### 2. æ­£æ–‡ç»“æ„ (4æ®µå¼ï¼Œæ¯æ®µçº¦150-200å­—ï¼Œç´§å‡‘æ’ç‰ˆ)
    **ç¬¬ä¸€æ®µï¼š{section1_title}**
    *   {section1_desc}
    *   **å…³é”®è¦æ±‚**ï¼šå¤šç”¨æ•°æ®æ”¯æ’‘ã€‚

    **ç¬¬äºŒæ®µï¼š{section2_title}**
    *   {section2_desc}

    **ç¬¬ä¸‰æ®µï¼š{section3_title}**
    *   {section3_desc}

    **ç¬¬å››æ®µï¼š{section4_title}**
    *   {section4_desc}

    **ç»“å°¾é¡µè„š (å¿…é¡»å®Œå…¨ä¸€è‡´)**
    æ­£æ–‡ç»“æŸåï¼Œè¯·è¾“å‡ºä»¥ä¸‹ HTML è¡¨æ ¼ä½œä¸ºé¡µè„šï¼ˆå…¨è“è‰²ï¼‰ï¼š
    ```html
    <br>
    <table style="width: 100%; border: none; color: blue; font-weight: bold; font-size: 14px;">
        <tr>
            <td style="text-align: left;">â€¢ è®¢é˜…è·¯å¾„ï¼šæ˜“æ·˜é‡‘APP-æŠ•é¡¾-ç¿ç»„åˆ-ç¿ç»„åˆxxå· (å°çº¢èŠ±)</td>
            <td style="text-align: right;">â€¢ è‚¡å¸‚æœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ï¼Œç»„åˆå»ºè®®ä»…ä¾›å‚è€ƒ</td>
        </tr>
    </table>
    ```

    ### 3. é¢œè‰²ä½¿ç”¨è§„åˆ™ (ä¸¥æ ¼æ‰§è¡Œ - å¿…é¡»å¤§é‡ä½¿ç”¨é¢œè‰²)
    **åŸåˆ™ï¼šé™¤äº†è¿æ¥è¯å’Œæ ‡ç‚¹ç¬¦å·ï¼Œå‡ ä¹æ‰€æœ‰å®è¯éƒ½åº”è¯¥ä¸Šè‰²ã€‚ä¸è¦è®©é»‘è‰²æ–‡å­—å æ®ä¸»å¯¼ã€‚**

    *   **<font color='red'>çº¢è‰² (Red) - ä»£è¡¨ç§¯æã€å¼ºåŠ¿ã€ä¸Šæ¶¨ã€çƒ­ç‚¹</font>**ï¼š
        *   **æ‰€æœ‰ä¸Šæ¶¨ç›¸å…³çš„åŠ¨è¯/å½¢å®¹è¯**ï¼šå¦‚ "ä¸Šæ¶¨", "æ”¶çº¢", "å¤§æ¶¨", "åˆ›æ–°é«˜", "ä¸ƒè¿é˜³", "åå¼¹", "ä¿®å¤", "æ´»è·ƒ", "çˆ†å‘", "èµ°å¼º", "å›å‡".
        *   **æ‰€æœ‰å¼ºåŠ¿æ¿å—å’Œä¸ªè‚¡åç§°**ï¼šå¦‚ "åŠå¯¼ä½“", "å®å¾·æ—¶ä»£", "å¤§æ¶ˆè´¹", "å•†ä¸šèˆªå¤©".
        *   **æ‰€æœ‰åˆ©å¥½å› ç´ **ï¼šå¦‚ "æ”¿ç­–çº¢åˆ©", "èµ„é‡‘æµå…¥", "ä¸šç»©è¶…é¢„æœŸ", "é‡ç»„", "çªç ´".
        *   **æ ¸å¿ƒè§‚ç‚¹/æœºä¼š**ï¼šå¦‚ "ç‰›å¸‚åˆæœŸ", "ç§¯æåšå¤š", "ä¸»çº¿", "ç»“æ„æ€§æœºä¼š".
        *   **å…³é”®æ­£å‘æ•°æ®**ï¼šå¦‚ "1.5ä¸‡äº¿", "è¶…4000å®¶".

    *   **<font color='blue'>è“è‰² (Blue) - ä»£è¡¨æ¶ˆæã€å¼±åŠ¿ã€ä¸‹è·Œã€é£é™©ã€å†·é™æè¿°</font>**ï¼š
        *   **æ‰€æœ‰ä¸‹è·Œç›¸å…³çš„åŠ¨è¯/å½¢å®¹è¯**ï¼šå¦‚ "ä¸‹è·Œ", "æ”¶è·Œ", "ç¿»ç»¿", "è°ƒæ•´", "å›è½", "è·³æ°´", "ä¸‹æŒ«", "æ‰¿å‹", "èµ°å¼±".
        *   **æ‰€æœ‰å¼±åŠ¿æ¿å—å’Œä¸ªè‚¡åç§°**ï¼šå¦‚ "åœ°äº§", "ç™½é…’" (å½“å®ƒä»¬ä¸‹è·Œæ—¶).
        *   **æ‰€æœ‰è´Ÿé¢/è°¨æ…å› ç´ **ï¼šå¦‚ "ç¼©é‡", "åˆ†æ­§", "æµå‡º", "å‡æŒ", "è§£ç¦", "åˆ©ç©º", "è§‚æœ›", "è°¨æ…".
        *   **ä¸­æ€§åç©ºçš„æè¿°**ï¼šå¦‚ "éœ‡è¡", "åˆ†åŒ–", "å­˜é‡åšå¼ˆ", "ç»“æ„æ€§", "è½®åŠ¨".
        *   **ç»“å°¾çš„è®¢é˜…è·¯å¾„å’Œé£é™©æç¤º**ã€‚

    *   **é»‘è‰² (Black)**ï¼š
        *   ä»…ç”¨äºè¿æ¥è¯ (çš„, äº†, æ˜¯, å’Œ)ã€æ ‡ç‚¹ç¬¦å·ã€ä»¥åŠéå¸¸æ™®é€šçš„å™è¿°æ€§æ–‡å­—ã€‚

    ### 4. å†™ä½œé£æ ¼
    *   **ç´§å‡‘å¯†é›†ä¸”å†…å®¹ä¸°å¯Œ**ï¼šä¸è¦åˆ†ç‚¹ï¼Œä¸è¦æ¢è¡Œå¤ªé¢‘ç¹ï¼Œåƒæ–°é—»é€šç¨¿ä¸€æ ·è¿è´¯ã€‚**æ¯æ®µå†…å®¹è¦å……å®ï¼Œå°½å¯èƒ½å¤šåœ°åŒ…å«ç»†èŠ‚ã€æ•°æ®å’Œé€»è¾‘åˆ†æï¼Œæ¨¡ä»¿ä¸“ä¸šæŠ•ç ”æŠ¥å‘Šçš„æ·±åº¦ã€‚**
    *   **ä¸“ä¸šæœ¯è¯­**ï¼šä½¿ç”¨â€œç»“æ„æ€§è¡Œæƒ…â€ã€â€œå­˜é‡åšå¼ˆâ€ã€â€œè·åˆ©ç›˜å…‘ç°â€ã€â€œæƒ…ç»ªå†°ç‚¹â€ç­‰ä¸“ä¸šè¯æ±‡ã€‚
    *   **æ•°æ®é©±åŠ¨**ï¼šå°½å¯èƒ½å¼•ç”¨è¾“å…¥æ•°æ®ä¸­çš„å…·ä½“æ•°å€¼ã€‚
    *   **é‡è¦ï¼šä¸è¦ä½¿ç”¨ Markdown ä»£ç å—**ã€‚è¯·ç›´æ¥è¾“å‡º HTML/Markdown æ··åˆæ–‡æœ¬ï¼Œä¸è¦ç”¨ ```html æˆ– ```markdown åŒ…è£¹ã€‚
    """

    user_prompt = f"ä»¥ä¸‹æ˜¯{prompt_role}çš„å¸‚åœºèµ„è®¯ç´ æï¼š\n\n{news_content}"

    try:
        response = client.chat.completions.create(
            model=AZURE_CONFIG["deploymentName"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=AZURE_CONFIG["temperature"],
            max_tokens=AZURE_CONFIG["maxTokens"]
        )
        content = response.choices[0].message.content
        
        # åå¤„ç†ï¼šç§»é™¤å¯èƒ½å­˜åœ¨çš„ Markdown ä»£ç å—æ ‡è®°
        if content.strip().startswith("```"):
            lines = content.strip().split('\n')
            # ç§»é™¤ç¬¬ä¸€è¡Œ (å¦‚ ```html)
            if lines[0].startswith("```"):
                lines = lines[1:]
            # ç§»é™¤æœ€åä¸€è¡Œ (å¦‚ ```)
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            content = "\n".join(lines)
            
        return content
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

def save_markdown(content, filename):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"æ™¨è®­ Markdown å·²ç”Ÿæˆ: {filename}")

def save_html(content, filename):
    # å°† Markdown è½¬æ¢ä¸º HTML
    html_body = markdown.markdown(content, extensions=['tables', 'fenced_code'])
    
    # æ„é€ å®Œæ•´çš„ HTMLï¼Œæ·»åŠ ä¸€äº›åŸºç¡€æ ·å¼ä»¥ä¼˜åŒ–é˜…è¯»ä½“éªŒ
    full_html = f"""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ç¿ç»„åˆå°çº¢èŠ±æ™¨è®¯</title>
        <style>
            body {{
                font-family: "Microsoft YaHei", "SimHei", sans-serif;
                line-height: 1.6;
                max_width: 800px;
                margin: 0 auto;
                padding: 20px;
                color: #333;
            }}
            table {{
                width: 100%;
                border-collapse: collapse;
            }}
            /* é’ˆå¯¹æ•°æ®æ¥æºçš„ä»£ç å—æ ·å¼ */
            pre {{
                background-color: #f5f5f5;
                padding: 15px;
                border-radius: 5px;
                overflow-x: auto;
                font-family: Consolas, monospace;
            }}
            /* é’ˆå¯¹ç”Ÿæˆçš„ HTML è¡¨æ ¼æ ·å¼ (å¦‚æœæœ‰) */
            td, th {{
                padding: 5px;
            }}
        </style>
    </head>
    <body>
        {html_body}
    </body>
    </html>
    """
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(full_html)
    print(f"æ™¨è®­ HTML å·²ç”Ÿæˆ: {filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç”Ÿæˆç¿ç»„åˆå°çº¢èŠ±æ™¨è®¯/å‘¨æŠ¥")
    parser.add_argument("--type", choices=["daily", "weekly"], default="daily", help="æŠ¥å‘Šç±»å‹: daily (æ—¥æŠ¥) æˆ– weekly (å‘¨æŠ¥)")
    args = parser.parse_args()
    
    report_type = args.type
    print(f"æ­£åœ¨ç”Ÿæˆ: {report_type} æŠ¥å‘Š")

    # 1. è·å–è‡ªåŠ¨æ•°æ®
    if report_type == "weekly":
        fetched_data = fetch_weekly_market_data()
    else:
        fetched_data = fetch_daily_market_data()
    
    # 2. è¯»å–æ‰‹åŠ¨è¡¥å……ç´ æ (å¯é€‰)
    manual_input = read_news_input("src/news_input.txt")
    
    final_content = ""
    if fetched_data:
        final_content += fetched_data + "\n\n"
    if manual_input:
        final_content += "ã€æ‰‹åŠ¨è¡¥å……ç´ æã€‘\n" + manual_input
        
    if not final_content.strip():
        print("æœªè·å–åˆ°ä»»ä½•æ•°æ® (AkShare å¤±è´¥ä¸”æ— æ‰‹åŠ¨è¾“å…¥)ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‰‹åŠ¨å¡«å…¥ src/news_input.txtã€‚")
    else:
        # 3. è°ƒç”¨ AI ç”Ÿæˆ
        print("æ­£åœ¨ç”ŸæˆæŠ¥å‘Šï¼Œè¯·ç¨å€™...")
        briefing_content = generate_briefing(final_content, report_type=report_type)
        
        # æ·»åŠ æ•°æ®æ¥æºæ¿å—
        if fetched_data:
            briefing_content += "\n\n---\n### æ•°æ®æ¥æº\n\n```text\n" + fetched_data + "\n```"

        # 4. ä¿å­˜æ–‡ä»¶
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # å®šä¹‰è¾“å‡ºç›®å½•ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ briefings æ–‡ä»¶å¤¹
        output_dir = os.path.join(os.path.dirname(script_dir), "briefings")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # ä¿å­˜ Markdown
        file_prefix = "Weekly" if report_type == "weekly" else "Briefing"
        md_file = os.path.join(output_dir, f"{datetime.datetime.now().strftime('%Y-%m-%d')}-{file_prefix}.md")
        save_markdown(briefing_content, md_file)
        
        # ä¿å­˜ HTML
        html_file = os.path.join(output_dir, f"{datetime.datetime.now().strftime('%Y-%m-%d')}-{file_prefix}.html")
        save_html(briefing_content, html_file)
