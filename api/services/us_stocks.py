"""
US Tech Stocks Data Service
ä½¿ç”¨Yahoo Financeè·å–ç¾è‚¡ç§‘æŠ€å·¨å¤´æ•°æ®
æ€§èƒ½ä¼˜åŒ–ï¼š
1. å¹¶å‘è·å–æ•°æ®ï¼ˆThreadPoolExecutorï¼‰
2. æ•°æ®ç¼“å­˜æœºåˆ¶ï¼ˆ1å°æ—¶è¿‡æœŸï¼‰
3. å¼‚å¸¸éš”ç¦»ï¼ˆå•è‚¡ç¥¨å¤±è´¥ä¸å½±å“æ•´ä½“ï¼‰
4. è¶…æ—¶æ§åˆ¶å’Œé™çº§ç­–ç•¥
"""

import yfinance as yf
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
import logging
import time

logger = logging.getLogger(__name__)

# è¿½è¸ªçš„9åªç§‘æŠ€å·¨å¤´ï¼ˆFAAMG + å…¶ä»–é¾™å¤´ï¼‰
US_TECH_STOCKS = {
    "AAPL": {"name": "è‹¹æœ", "name_en": "Apple", "emoji": "ğŸ"},
    "MSFT": {"name": "å¾®è½¯", "name_en": "Microsoft", "emoji": "â“‚ï¸"},
    "GOOGL": {"name": "è°·æ­Œ", "name_en": "Alphabet", "emoji": "ğŸ”"},
    "AMZN": {"name": "äºšé©¬é€Š", "name_en": "Amazon", "emoji": "ğŸ›’"},
    "META": {"name": "Meta", "name_en": "Meta Platforms", "emoji": "ğŸ“˜"},
    "NVDA": {"name": "è‹±ä¼Ÿè¾¾", "name_en": "NVIDIA", "emoji": "ğŸ’»"},
    "TSLA": {"name": "ç‰¹æ–¯æ‹‰", "name_en": "Tesla", "emoji": "âš¡"},
    "NFLX": {"name": "å¥ˆé£", "name_en": "Netflix", "emoji": "ğŸ¬"},
    "AMD": {"name": "AMD", "name_en": "AMD", "emoji": "ğŸ”§"}
}

# ç¼“å­˜é…ç½®
CACHE_DURATION = 3600  # 1å°æ—¶ç¼“å­˜
_memory_cache = {}  # å†…å­˜ç¼“å­˜

# å¸‚å€¼ç‹¬ç«‹ç¼“å­˜ï¼ˆ24å°æ—¶æœ‰æ•ˆï¼Œå¸‚å€¼å˜åŒ–ä¸å¤§ï¼‰
_market_cap_cache = {}  # {symbol: (market_cap_value, timestamp)}
_MARKET_CAP_CACHE_DURATION = 86400  # 24 hours


def get_stock_data(symbol: str, use_cache: bool = True) -> Optional[Dict]:
    """
    è·å–å•åªè‚¡ç¥¨æ•°æ®ï¼ˆå¸¦ç¼“å­˜å’Œå®¹é”™ï¼‰
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
    """
    # æ£€æŸ¥å†…å­˜ç¼“å­˜
    if use_cache and symbol in _memory_cache:
        cache_data, cache_time = _memory_cache[symbol]
        if datetime.now() - cache_time < timedelta(seconds=CACHE_DURATION):
            logger.info(f"{symbol} ä½¿ç”¨ç¼“å­˜æ•°æ®")
            cache_data['from_cache'] = True
            return cache_data

    try:
        logger.info(f"å¼€å§‹è·å– {symbol} æ•°æ®...")
        start_time = time.time()

        ticker = yf.Ticker(symbol)

        # è·å–å†å²æ•°æ®ï¼ˆæœ€è¿‘30å¤©ï¼‰ï¼Œè®¾ç½®è¶…æ—¶
        hist = ticker.history(period="30d", timeout=10)

        if hist.empty:
            logger.warning(f"{symbol} å†å²æ•°æ®ä¸ºç©º")
            return _get_cached_or_error(symbol)

        # æœ€æ–°å’Œå‰ä¸€äº¤æ˜“æ—¥æ•°æ®
        latest = hist.iloc[-1]
        prev = hist.iloc[-2] if len(hist) > 1 else latest

        # è®¡ç®—æ¶¨è·Œ
        current_price = latest['Close']
        prev_close = prev['Close']
        change = current_price - prev_close
        change_percent = (change / prev_close) * 100

        # è¶‹åŠ¿æ•°æ®ï¼ˆæœ€è¿‘30å¤©æ”¶ç›˜ä»·ï¼‰
        trend_data = hist['Close'].tolist()

        stock_info = US_TECH_STOCKS.get(symbol, {"name": symbol, "name_en": symbol, "emoji": "ğŸ“Š"})

        # è·å–é¢å¤–ä¿¡æ¯ï¼ˆä½¿ç”¨å¿«é€Ÿæ–¹æ³•ï¼Œé¿å…æ…¢é€ŸAPIè°ƒç”¨ï¼‰
        try:
            info = ticker.fast_info  # ä½¿ç”¨fast_infoä»£æ›¿infoï¼ˆæ›´å¿«ï¼‰
            market_cap = info.last_price * info.shares if hasattr(info, 'shares') else 0
        except:
            # é™çº§åˆ°æ™®é€šinfo
            try:
                info_dict = ticker.info
                market_cap = info_dict.get('marketCap', 0)
            except:
                market_cap = 0

        elapsed = time.time() - start_time
        logger.info(f"{symbol} æ•°æ®è·å–å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’")

        result = {
            "symbol": symbol,
            "name": stock_info["name"],
            "name_en": stock_info["name_en"],
            "emoji": stock_info.get("emoji", "ğŸ“Š"),
            "price": round(current_price, 2),
            "change": round(change, 2),
            "change_percent": round(change_percent, 2),
            "open": round(latest['Open'], 2),
            "high": round(latest['High'], 2),
            "low": round(latest['Low'], 2),
            "close": round(latest['Close'], 2),
            "volume": int(latest['Volume']),
            "volume_str": f"{int(latest['Volume']/1000000)}M" if latest['Volume'] > 1000000 else f"{int(latest['Volume']/1000)}K",
            "trend": [round(p, 2) for p in trend_data[-30:]],
            "market_cap": market_cap,
            "market_cap_str": _format_market_cap(market_cap),
            "updated_at": datetime.now().isoformat(),
            "from_cache": False,
            "data_source": "yahoo_finance"
        }

        # å­˜å…¥ç¼“å­˜
        _memory_cache[symbol] = (result, datetime.now())

        return result

    except Exception as e:
        logger.error(f"{symbol} è·å–æ•°æ®å¤±è´¥: {e}")
        return _get_cached_or_error(symbol)


def _get_cached_or_error(symbol: str) -> Optional[Dict]:
    """è·å–ç¼“å­˜æ•°æ®æˆ–è¿”å›é”™è¯¯å ä½ç¬¦"""
    # å°è¯•ä»ç¼“å­˜è·å–ï¼ˆå³ä½¿è¿‡æœŸï¼‰
    if symbol in _memory_cache:
        cache_data, _ = _memory_cache[symbol]
        cache_data['from_cache'] = True
        cache_data['is_stale'] = True
        logger.warning(f"{symbol} ä½¿ç”¨è¿‡æœŸç¼“å­˜æ•°æ®")
        return cache_data

    # è¿”å›é”™è¯¯å ä½ç¬¦
    stock_info = US_TECH_STOCKS.get(symbol, {"name": symbol, "name_en": symbol, "emoji": "âŒ"})
    return {
        "symbol": symbol,
        "name": stock_info["name"],
        "name_en": stock_info["name_en"],
        "emoji": stock_info.get("emoji", "âŒ"),
        "error": "æ•°æ®è·å–å¤±è´¥",
        "data_source": "error"
    }


def _format_market_cap(market_cap: float) -> str:
    """æ ¼å¼åŒ–å¸‚å€¼æ˜¾ç¤º"""
    if market_cap >= 1e12:
        return f"${round(market_cap/1e12, 2)}T"
    elif market_cap >= 1e9:
        return f"${round(market_cap/1e9, 2)}B"
    elif market_cap >= 1e6:
        return f"${round(market_cap/1e6, 2)}M"
    else:
        return "N/A"


def get_us_tech_overview(use_cache: bool = True, max_workers: int = 5) -> Dict:
    """
    è·å–æ‰€æœ‰ç§‘æŠ€è‚¡çš„æ¦‚è§ˆæ•°æ®ï¼ˆæ‰¹é‡ä¸‹è½½ä¼˜åŒ–ï¼‰
    Args:
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆç”¨äºè·å– market_cap ç­‰è¡¥å……æ•°æ®ï¼‰
    """
    logger.info(f"å¼€å§‹è·å–ç¾è‚¡ç§‘æŠ€è‚¡æ•°æ®...")
    start_time = time.time()

    stocks_data = []
    symbols = list(US_TECH_STOCKS.keys())

    # Check if all symbols are cached
    if use_cache:
        all_cached = True
        for symbol in symbols:
            if symbol not in _memory_cache:
                all_cached = False
                break
            cache_data, cache_time = _memory_cache[symbol]
            if datetime.now() - cache_time >= timedelta(seconds=CACHE_DURATION):
                all_cached = False
                break

        if all_cached:
            logger.info("æ‰€æœ‰è‚¡ç¥¨å‡å‘½ä¸­ç¼“å­˜")
            for symbol in symbols:
                cache_data, _ = _memory_cache[symbol]
                cache_data_copy = dict(cache_data)
                cache_data_copy['from_cache'] = True
                stocks_data.append(cache_data_copy)
            return _build_overview_result(stocks_data, start_time)

    # Batch download all tickers at once (single request to yfinance)
    try:
        logger.info(f"æ‰¹é‡ä¸‹è½½ {len(symbols)} åªè‚¡ç¥¨æ•°æ®...")
        tickers_str = " ".join(symbols)
        hist_data = yf.download(tickers_str, period="30d", group_by="ticker", threads=True, timeout=20)

        if hist_data is not None and not hist_data.empty:
            for symbol in symbols:
                try:
                    # Extract per-symbol data from the batch result
                    if len(symbols) > 1:
                        sym_hist = hist_data[symbol].dropna(how='all')
                    else:
                        sym_hist = hist_data.dropna(how='all')

                    if sym_hist.empty or len(sym_hist) < 2:
                        logger.warning(f"{symbol} æ‰¹é‡ä¸‹è½½æ•°æ®ä¸è¶³")
                        stocks_data.append(_get_cached_or_error(symbol))
                        continue

                    latest = sym_hist.iloc[-1]
                    prev = sym_hist.iloc[-2]

                    current_price = float(latest['Close'])
                    prev_close = float(prev['Close'])
                    change = current_price - prev_close
                    change_percent = (change / prev_close) * 100

                    trend_data = sym_hist['Close'].tolist()

                    stock_info = US_TECH_STOCKS.get(symbol, {"name": symbol, "name_en": symbol, "emoji": "ğŸ“Š"})

                    result = {
                        "symbol": symbol,
                        "name": stock_info["name"],
                        "name_en": stock_info["name_en"],
                        "emoji": stock_info.get("emoji", "ğŸ“Š"),
                        "price": round(current_price, 2),
                        "change": round(change, 2),
                        "change_percent": round(change_percent, 2),
                        "open": round(float(latest['Open']), 2),
                        "high": round(float(latest['High']), 2),
                        "low": round(float(latest['Low']), 2),
                        "close": round(float(latest['Close']), 2),
                        "volume": int(latest['Volume']),
                        "volume_str": f"{int(latest['Volume']/1000000)}M" if latest['Volume'] > 1000000 else f"{int(latest['Volume']/1000)}K",
                        "trend": [round(float(p), 2) for p in trend_data[-30:]],
                        "market_cap": 0,
                        "market_cap_str": "N/A",
                        "updated_at": datetime.now().isoformat(),
                        "from_cache": False,
                        "data_source": "yahoo_finance"
                    }

                    # Cache the result
                    _memory_cache[symbol] = (result, datetime.now())
                    stocks_data.append(result)

                except Exception as e:
                    logger.error(f"{symbol} è§£ææ‰¹é‡æ•°æ®å¤±è´¥: {e}")
                    stocks_data.append(_get_cached_or_error(symbol))
        else:
            logger.warning("æ‰¹é‡ä¸‹è½½è¿”å›ç©ºæ•°æ®ï¼Œå›é€€åˆ°é€ä¸ªè·å–")
            raise Exception("Batch download returned empty data")

    except Exception as e:
        logger.warning(f"æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}ï¼Œå›é€€åˆ°é€ä¸ªè·å–")
        # Fallback: fetch individually with ThreadPoolExecutor
        stocks_data = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {
                executor.submit(get_stock_data, symbol, use_cache): symbol
                for symbol in symbols
            }
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    data = future.result(timeout=15)
                    if data:
                        stocks_data.append(data)
                except Exception as ex:
                    logger.error(f"{symbol} è·å–å¼‚å¸¸: {ex}")
                    stocks_data.append(_get_cached_or_error(symbol))

    # Try to enrich market_cap in background (non-critical)
    try:
        _enrich_market_caps(stocks_data, max_workers)
    except Exception as e:
        logger.warning(f"å¸‚å€¼æ•°æ®è¡¥å……å¤±è´¥ï¼ˆä¸å½±å“ä¸»æ•°æ®ï¼‰: {e}")

    return _build_overview_result(stocks_data, start_time)


def _enrich_market_caps(stocks_data: List[Dict], max_workers: int = 3):
    """Enrich stocks with market cap data (best effort, won't fail the main flow).
    Uses a dedicated 24-hour cache to avoid redundant API calls."""
    now_ts = time.time()
    symbols_needing_cap = []

    for s in stocks_data:
        if 'error' in s:
            continue
        sym = s['symbol']
        # Check dedicated market_cap cache first
        if sym in _market_cap_cache:
            cached_cap, cached_ts = _market_cap_cache[sym]
            if now_ts - cached_ts < _MARKET_CAP_CACHE_DURATION:
                s['market_cap'] = cached_cap
                s['market_cap_str'] = _format_market_cap(cached_cap)
                # Also update main cache
                if sym in _memory_cache:
                    cached, ts = _memory_cache[sym]
                    cached['market_cap'] = cached_cap
                    cached['market_cap_str'] = s['market_cap_str']
                continue
        if s.get('market_cap', 0) == 0:
            symbols_needing_cap.append(s)

    if not symbols_needing_cap:
        return

    def fetch_cap(stock):
        try:
            ticker = yf.Ticker(stock['symbol'])
            info = ticker.fast_info
            market_cap = info.last_price * info.shares if hasattr(info, 'shares') else 0
            stock['market_cap'] = market_cap
            stock['market_cap_str'] = _format_market_cap(market_cap)
            # Store in dedicated market_cap cache
            _market_cap_cache[stock['symbol']] = (market_cap, time.time())
            # Update main cache
            if stock['symbol'] in _memory_cache:
                cached, ts = _memory_cache[stock['symbol']]
                cached['market_cap'] = market_cap
                cached['market_cap_str'] = stock['market_cap_str']
        except Exception:
            pass

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        list(executor.map(fetch_cap, symbols_needing_cap))


def _build_overview_result(stocks_data: List[Dict], start_time: float) -> Dict:
    """Build the final overview result dict from stocks data."""
    valid_stocks = [s for s in stocks_data if 'error' not in s]

    if valid_stocks:
        avg_change = sum(s['change_percent'] for s in valid_stocks) / len(valid_stocks)
        up_count = sum(1 for s in valid_stocks if s['change_percent'] > 0)
        down_count = sum(1 for s in valid_stocks if s['change_percent'] < 0)
        flat_count = len(valid_stocks) - up_count - down_count

        top_gainer = max(valid_stocks, key=lambda x: x['change_percent'])
        top_loser = min(valid_stocks, key=lambda x: x['change_percent'])
    else:
        avg_change = 0
        up_count = 0
        down_count = 0
        flat_count = 0
        top_gainer = None
        top_loser = None

    elapsed_time = time.time() - start_time
    logger.info(f"ç¾è‚¡æ•°æ®è·å–å®Œæˆï¼ŒæˆåŠŸ {len(valid_stocks)}/{len(US_TECH_STOCKS)}ï¼Œè€—æ—¶ {elapsed_time:.2f}ç§’")

    return {
        "stocks": stocks_data,
        "summary": {
            "total": len(stocks_data),
            "success": len(valid_stocks),
            "up": up_count,
            "down": down_count,
            "flat": flat_count,
            "avg_change": round(avg_change, 2),
            "top_gainer": {
                "symbol": top_gainer['symbol'],
                "name": top_gainer['name'],
                "change_percent": top_gainer['change_percent']
            } if top_gainer else None,
            "top_loser": {
                "symbol": top_loser['symbol'],
                "name": top_loser['name'],
                "change_percent": top_loser['change_percent']
            } if top_loser else None
        },
        "updated_at": datetime.now().isoformat(),
        "elapsed_time": round(elapsed_time, 2)
    }


def save_us_tech_data(data: Dict):
    """ä¿å­˜ç¾è‚¡æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        # è·å–storageç›®å½•
        storage_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "storage")
        os.makedirs(storage_dir, exist_ok=True)

        today = datetime.now().strftime("%Y-%m-%d")
        filename = f"{today}_us_tech.json"
        filepath = os.path.join(storage_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"US tech data saved to {filepath}")
        return True
    except Exception as e:
        logger.error(f"Error saving US tech data: {e}")
        return False


def load_us_tech_data(date: Optional[str] = None) -> Optional[Dict]:
    """ä»æ–‡ä»¶åŠ è½½ç¾è‚¡æ•°æ®"""
    try:
        storage_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "storage")

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨ä»Šå¤©
        if not date:
            date = datetime.now().strftime("%Y-%m-%d")

        filename = f"{date}_us_tech.json"
        filepath = os.path.join(storage_dir, filename)

        if os.path.exists(filepath):
            with open(filepath, "r", encoding="utf-8") as f:
                return json.load(f)

        logger.warning(f"US tech data file not found: {filepath}")
        return None
    except Exception as e:
        logger.error(f"Error loading US tech data: {e}")
        return None


def clear_cache():
    """æ¸…ç©ºå†…å­˜ç¼“å­˜"""
    global _memory_cache
    _memory_cache = {}
    logger.info("ç¾è‚¡æ•°æ®ç¼“å­˜å·²æ¸…ç©º")


def get_cache_stats() -> Dict:
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    now = datetime.now()
    stats = {
        "total_cached": len(_memory_cache),
        "cached_symbols": list(_memory_cache.keys()),
        "cache_ages": {}
    }

    for symbol, (_, cache_time) in _memory_cache.items():
        age_seconds = (now - cache_time).total_seconds()
        stats["cache_ages"][symbol] = {
            "age_seconds": int(age_seconds),
            "is_fresh": age_seconds < CACHE_DURATION
        }

    return stats


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    data = get_us_tech_overview(use_cache=False)
    print(json.dumps(data, ensure_ascii=False, indent=2))
    save_us_tech_data(data)
